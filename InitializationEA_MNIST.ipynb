{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InitializationEA_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPe+rBO5XKs3fbPseJFumaV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samisihem/testinit/blob/master/InitializationEA_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xy2Od5HptAzx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "outputId": "4498b77f-a014-4456-b8b9-617ef71bbcbb"
      },
      "source": [
        "# Code Authors: Pan Ji,     University of Adelaide,         pan.ji@adelaide.edu.au\n",
        "#               Tong Zhang, Australian National University, tong.zhang@anu.edu.au\n",
        "# Copyright Reserved!\n",
        "#!git clone https://github.com/tensorflow/tensorflow.git\n",
        "#!pip uninstall tensorflow -y\n",
        "#!pip install  tensorflow==1.14\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.contrib import layers\n",
        "#import matlab.engine\n",
        "import scipy.io as sio\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import os\n",
        "from sklearn.cluster import KMeans\n",
        "from softkmeans import *\n",
        "from scipy.linalg import svd\n",
        "import sklearn.metrics as metrics\n",
        "from Poweriteration import *\n",
        "# SELECT GPU\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
        "\n",
        "def next_batch(data, _index_in_epoch ,batch_size , _epochs_completed):\n",
        "    _num_examples = data.shape[0]\n",
        "    start = _index_in_epoch\n",
        "    _index_in_epoch += batch_size\n",
        "    if _index_in_epoch > _num_examples:\n",
        "        # Finished epoch\n",
        "        _epochs_completed += 1\n",
        "        # Shuffle the data\n",
        "        perm = np.arange(_num_examples)\n",
        "        np.random.shuffle(perm)\n",
        "        data = data[perm]\n",
        "        #label = label[perm]\n",
        "        # Start next epoch\n",
        "        start = 0\n",
        "        _index_in_epoch = batch_size\n",
        "        assert batch_size <= _num_examples\n",
        "    end = _index_in_epoch\n",
        "    return data[start:end], _index_in_epoch, _epochs_completed\n",
        "\n",
        "class ConvAE(object):\n",
        "    def __init__(self, n_input, kernel_size,n_hidden, learning_rate = 1e-3, batch_size = 256,\\\n",
        "        reg = None, denoise = False ,model_path = None,restore_path = None, logs_path = './models_face'):\n",
        "    #n_hidden is a arrary contains the number of neurals on every layer\n",
        "        self.n_input = n_input\n",
        "        self.n_hidden = n_hidden\n",
        "        self.reg = reg\n",
        "        self.model_path = model_path\n",
        "        self.restore_path = restore_path\n",
        "        self.kernel_size = kernel_size\n",
        "        self.batch_size = batch_size\n",
        "        self.iter = 0\n",
        "        weights = self._initialize_weights()\n",
        "        \n",
        "        # model\n",
        "        self.x = tf.placeholder(tf.float32, [None, self.n_input[0], self.n_input[1], 1])        \n",
        "\n",
        "        if denoise == False:\n",
        "            x_input = self.x\n",
        "            latent, shape = self.encoder(x_input, weights)\n",
        "\n",
        "        else:\n",
        "            x_input = tf.add(self.x, tf.random_normal(shape=tf.shape(self.x),\n",
        "                                               mean = 0,\n",
        "                                               stddev = 0.2,\n",
        "                                               dtype=tf.float32))\n",
        "\n",
        "            latent,shape = self.encoder(x_input, weights)\n",
        "        self.z = tf.reshape(latent,[batch_size, -1])\n",
        "        self.x_r = self.decoder(latent, weights, shape)\n",
        "        self.saver = tf.train.Saver()\n",
        "        # cost for reconstruction\n",
        "        # l_2 loss \n",
        "        self.cost = 0.5 * tf.reduce_sum(tf.pow(tf.subtract(self.x_r, self.x), 2.0))   # choose crossentropy or l2 loss\n",
        "        tf.summary.scalar(\"l2_loss\", self.cost)          \n",
        "        \n",
        "        self.merged_summary_op = tf.summary.merge_all()        \n",
        "        \n",
        "        self.loss = self.cost\n",
        "\n",
        "        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.loss) #GradientDescentOptimizer #AdamOptimizer\n",
        "        init = tf.global_variables_initializer()\n",
        "        self.sess = tf.InteractiveSession()\n",
        "        self.sess.run(init)\n",
        "        self.summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        all_weights = dict()\n",
        "        n_layers = len(self.n_hidden)\n",
        "        all_weights['Coef']   = tf.Variable(0 * tf.ones([self.batch_size, self.batch_size],tf.float32), name = 'Coef')        \n",
        "        \n",
        "        all_weights['enc_w0'] = tf.get_variable(\"enc_w0\", shape=[self.kernel_size[0], self.kernel_size[0], 1, self.n_hidden[0]],\n",
        "            initializer=layers.xavier_initializer_conv2d(),regularizer = self.reg)\n",
        "        all_weights['enc_b0'] = tf.Variable(tf.zeros([self.n_hidden[0]], dtype = tf.float32)) # , name = 'enc_b0'\n",
        "        \n",
        "        iter_i = 1\n",
        "        while iter_i < n_layers:\n",
        "            enc_name_wi = 'enc_w' + str(iter_i)\n",
        "            all_weights[enc_name_wi] = tf.get_variable(enc_name_wi, shape=[self.kernel_size[iter_i], self.kernel_size[iter_i], self.n_hidden[iter_i-1], \\\n",
        "                        self.n_hidden[iter_i]], initializer=layers.xavier_initializer_conv2d(),regularizer = self.reg)\n",
        "            enc_name_bi = 'enc_b' + str(iter_i)\n",
        "            all_weights[enc_name_bi] = tf.Variable(tf.zeros([self.n_hidden[iter_i]], dtype = tf.float32)) # , name = enc_name_bi\n",
        "            iter_i = iter_i + 1\n",
        "        \n",
        "        iter_i = 1\n",
        "        while iter_i < n_layers:    \n",
        "            dec_name_wi = 'dec_w' + str(iter_i - 1)\n",
        "            all_weights[dec_name_wi] = tf.get_variable(dec_name_wi, shape=[self.kernel_size[n_layers-iter_i], self.kernel_size[n_layers-iter_i], \n",
        "                        self.n_hidden[n_layers-iter_i-1],self.n_hidden[n_layers-iter_i]], initializer=layers.xavier_initializer_conv2d(),regularizer = self.reg)\n",
        "            dec_name_bi = 'dec_b' + str(iter_i - 1)\n",
        "            all_weights[dec_name_bi] = tf.Variable(tf.zeros([self.n_hidden[n_layers-iter_i-1]], dtype = tf.float32)) # , name = dec_name_bi\n",
        "            iter_i = iter_i + 1\n",
        "            \n",
        "        dec_name_wi = 'dec_w' + str(iter_i - 1)\n",
        "        all_weights[dec_name_wi] = tf.get_variable(dec_name_wi, shape=[self.kernel_size[0], self.kernel_size[0],1, self.n_hidden[0]],\n",
        "            initializer=layers.xavier_initializer_conv2d(),regularizer = self.reg)\n",
        "        dec_name_bi = 'dec_b' + str(iter_i - 1)\n",
        "        all_weights[dec_name_bi] = tf.Variable(tf.zeros([1], dtype = tf.float32)) # , name = dec_name_bi\n",
        "        \n",
        "        return all_weights\n",
        "        \n",
        "    # Building the encoder\n",
        "    def encoder(self,x, weights):\n",
        "        shapes = []\n",
        "        shapes.append(x.get_shape().as_list())\n",
        "        layeri = tf.nn.bias_add(tf.nn.conv2d(x, weights['enc_w0'], strides=[1,2,2,1],padding='SAME'),weights['enc_b0'])\n",
        "        layeri = tf.nn.relu(layeri)\n",
        "        shapes.append(layeri.get_shape().as_list())\n",
        "        \n",
        "        n_layers = len(self.n_hidden)\n",
        "        iter_i = 1\n",
        "        while iter_i < n_layers:\n",
        "            layeri = tf.nn.bias_add(tf.nn.conv2d(layeri, weights['enc_w' + str(iter_i)], strides=[1,2,2,1],padding='SAME'),weights['enc_b' + str(iter_i)])\n",
        "            layeri = tf.nn.relu(layeri)\n",
        "            shapes.append(layeri.get_shape().as_list())\n",
        "            iter_i = iter_i + 1\n",
        "        \n",
        "        layer3 = layeri\n",
        "        return  layer3, shapes\n",
        "    \n",
        "    # Building the decoder\n",
        "    def decoder(self,z, weights, shapes):\n",
        "        n_layers = len(self.n_hidden)        \n",
        "        layer3 = z\n",
        "        iter_i = 0\n",
        "        while iter_i < n_layers:\n",
        "            #if iter_i == n_layers-1:\n",
        "            #    strides_i = [1,2,2,1]\n",
        "            #else:\n",
        "            #    strides_i = [1,1,1,1]\n",
        "            shape_de = shapes[n_layers - iter_i - 1]            \n",
        "            layer3 = tf.add(tf.nn.conv2d_transpose(layer3, weights['dec_w' + str(iter_i)], tf.stack([tf.shape(self.x)[0],shape_de[1],shape_de[2],shape_de[3]]),\\\n",
        "                     strides=[1,2,2,1],padding='SAME'), weights['dec_b' + str(iter_i)])\n",
        "            layer3 = tf.nn.relu(layer3)\n",
        "            iter_i = iter_i + 1\n",
        "        return layer3\n",
        "\n",
        "    def partial_fit(self, X): \n",
        "        cost, summary, _ = self.sess.run((self.cost, self.merged_summary_op, self.optimizer), feed_dict = {self.x: X})\n",
        "        self.summary_writer.add_summary(summary, self.iter)\n",
        "        self.iter = self.iter + 1\n",
        "        return cost \n",
        "\n",
        "    def reconstruct(self,X):\n",
        "        return self.sess.run(self.x_r, feed_dict = {self.x:X})\n",
        "\n",
        "    def transform(self, X):\n",
        "        return self.sess.run(self.z, feed_dict = {self.x:X})\n",
        "\n",
        "    def save_model(self):\n",
        "        save_path = self.saver.save(self.sess,self.model_path)\n",
        "        print (\"model saved in file: %s\" % save_path)\n",
        "\n",
        "    def restore(self):\n",
        "        self.saver.restore(self.sess, self.restore_path)\n",
        "        print (\"model restored\")\n",
        "\n",
        "def ae_feature_clustering(CAE, X):\n",
        "    CAE.restore()\n",
        "    \n",
        "    #eng = matlab.engine.start_matlab()\n",
        "    #eng.addpath(r'/home/pan/workspace-eclipse/deep-subspace-clustering/SSC_ADMM_v1.1',nargout=0)\n",
        "    #eng.addpath(r'/home/pan/workspace-eclipse/deep-subspace-clustering/EDSC_release',nargout=0)\n",
        "    \n",
        "    Z = CAE.transform(X)\n",
        "    \n",
        "    #sio.savemat('AE_YaleB.mat', dict(Z = Z) )\n",
        "    \n",
        "    return Z\n",
        "\n",
        "def train_face(Img, CAE, n_input, batch_size):    \n",
        "    it = 0\n",
        "    display_step = 300\n",
        "    save_step = 900\n",
        "    _index_in_epoch = 0\n",
        "    _epochs= 0\n",
        "\n",
        "    # CAE.restore()\n",
        "    # train the network\n",
        "    while True:\n",
        "        batch_x,  _index_in_epoch, _epochs =  next_batch(Img, _index_in_epoch , batch_size , _epochs)\n",
        "        batch_x = np.reshape(batch_x,[batch_size,n_input[0],n_input[1],1])\n",
        "        cost = CAE.partial_fit(batch_x)\n",
        "        it = it +1\n",
        "        avg_cost = cost/(batch_size)\n",
        "        if it % display_step == 0:\n",
        "            print (\"epoch: %.1d\" % _epochs)\n",
        "            print  (\"cost: %.8f\" % avg_cost)\n",
        "        if it % save_step == 0:\n",
        "            CAE.save_model()\n",
        "    return\n",
        "\n",
        "def test_face(Img, CAE, n_input):\n",
        "    \n",
        "    batch_x_test = Img[200:300,:]\n",
        "    batch_x_test= np.reshape(batch_x_test,[100,n_input[0],n_input[1],1])\n",
        "    CAE.restore()\n",
        "    x_re = CAE.reconstruct(batch_x_test)\n",
        "\n",
        "    plt.figure(figsize=(8,12))\n",
        "    for i in range(5):\n",
        "        plt.subplot(5,2,2*i+1)\n",
        "        plt.imshow(batch_x_test[i,:,:,0], vmin=0, vmax=255, cmap=\"gray\") #\n",
        "        plt.title(\"Test input\")\n",
        "        plt.colorbar()\n",
        "        plt.subplot(5, 2, 2*i + 2)\n",
        "        plt.imshow(x_re[i,:,:,0], vmin=0, vmax=255, cmap=\"gray\")\n",
        "        plt.title(\"Reconstruction\")\n",
        "        plt.colorbar()\n",
        "        plt.tight_layout()\n",
        "    plt.show()\n",
        "    return\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    #data = sio.loadmat('./Data//ORL_32x32.mat')\n",
        "    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=False)\n",
        "    Img = []\n",
        "    Label = []\n",
        "    num = mnist.train.num_examples\n",
        "    rawImg = mnist.train._images\n",
        "    rawLabel = mnist.train._labels\n",
        "    for i in range(10):\n",
        "        ind = [ii for ii in range(num) if rawLabel[ii] == i]\n",
        "        ind = ind[0:100]\n",
        "        if i == 0:\n",
        "            Img = rawImg[ind]\n",
        "            Label = rawLabel[ind]\n",
        "        else:\n",
        "            Img = np.concatenate([Img,rawImg[ind]])\n",
        "            Label =  np.concatenate([Label,rawLabel[ind]])\n",
        "    Label = np.reshape(Label, (-1, 1))\n",
        "    \n",
        "    n_input = [28, 28]\n",
        "    n_hidden = [20, 10, 5]\n",
        "    kernel_size = [5,3,3]\n",
        "\n",
        "    Img = np.reshape(Img,[Img.shape[0],n_input[0],n_input[1],1]) \n",
        "\n",
        "    batch_size = Img.shape[0]    \n",
        "    lr = 1.0e-3 # learning rate\n",
        "    model_path = './models/model-mnist.ckpt'\n",
        "    CAE = ConvAE(n_input = n_input, n_hidden = n_hidden, learning_rate = lr, kernel_size = kernel_size, \n",
        "                 batch_size = batch_size, model_path = model_path, restore_path = model_path)\n",
        "    #test_face(Img, CAE, n_input)\n",
        "    #train_face(Img, CAE, n_input, batch_size)\n",
        "    X = np.reshape(Img, [Img.shape[0],n_input[0],n_input[1],1])\n",
        "    Z=ae_feature_clustering(CAE, X)\n",
        "\n",
        "    #soft kmeans#####################################################\n",
        "    P=soft_k_means(Z,100,3)#|Z|=1000,100 landmarks points and 3 nearest nighborhood\n",
        "    #print(\"P shape\\n\",P.shape)\n",
        "    #print(\"initialisation P\\n\",P)    \n",
        "    #P=P.dot(np.transpose(P))    \n",
        "\n",
        "    #Compute Q SVD(P)#################################################\n",
        "    U,S,V=svd(P)\n",
        "    Q=U\n",
        "    kmeans=KMeans(n_clusters=10, random_state=0).fit(Q)    \n",
        "    #print(kmeans.labels_) \n",
        "    #print(kmeans.cluster_centers_)\n",
        "      \n",
        "    score = metrics.accuracy_score(Label,kmeans.labels_)\n",
        "    print('Accuracy:{0:f}'.format(score))   \n",
        "    \n",
        "    ##################################################################\n",
        "    "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-1-fa76a98bcad2>:236: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from ./models/model-mnist.ckpt\n",
            "model restored\n",
            "Accuracy:0.097000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RLLiNb-To0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def Compute_P(centers, x, beta,V):\n",
        "    N, _ = x.shape\n",
        "    K, D = centers.shape\n",
        "    P = np.zeros((N, K))\n",
        "    Distances = np.zeros((K, 2))  \n",
        "    for n in range(N): \n",
        "      #Distances between a point and the k centers     \n",
        "      for l in range(K):\n",
        "        Distances[l,0]=(x[n]-centers[l]).mean()\n",
        "        Distances[l,1]=l      \n",
        "      #sort by distances\n",
        "      Distances = Distances[Distances[:,0].argsort()]       \n",
        "      #compute for the 3 nearest nighborhood       \n",
        "      for j in range(V):                         \n",
        "        P[n,int(Distances[j,1]) ] = np.exp(-beta * np.linalg.norm(centers[int(Distances[j,1])] - x[n], 2))        \n",
        "      P[n] /= P[n].sum()\n",
        "    return P\n",
        "\n",
        "def soft_k_means(x, K,V, beta=1.):    \n",
        "    centers = KMeans(n_clusters=K,random_state=0).fit(x).cluster_centers_    \n",
        "    p = Compute_P(centers, x, beta,V)    \n",
        "    return p    \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKPFNwfiX_Xx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "import time\n",
        "\n",
        "def power_iteration(A, niter):\n",
        "  tol = 10**(-9)\n",
        "  n,m = A.shape\n",
        "  eigvec = np.random.rand(int(n))\n",
        "  eigval_old = np.dot(np.transpose(eigvec),A.dot(eigvec))/np.dot(np.transpose(eigvec),eigvec)\n",
        "  for i in range(niter):\n",
        "    # calculate the matrix-by-vector product Ab\n",
        "    eigvec1 = A.dot(eigvec)\n",
        "    # calculate the norm\n",
        "    eigvec1_norm = np.linalg.norm(eigvec1)\n",
        "    # re normalize the vector\n",
        "    eigvec = eigvec1 / eigvec1_norm\n",
        "\t\t#eigenvalue\n",
        "    eigval_new = np.dot(np.transpose(eigvec),A.dot(eigvec))/np.dot(np.transpose(eigvec),eigvec)\n",
        "    if (abs(eigval_new-eigval_old)/eigval_new) < tol:\n",
        "      return eigval_new\n",
        "    eigval_old = eigval_new\n",
        "\t\t\n",
        "  return eigval_new"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}